<!doctype html><html lang=zh-cn>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=robots content="noodp">
<meta http-equiv=x-ua-compatible content="IE=edge, chrome=1">
<title>DataParallel the Dimension of Output Is Different From Input - majiabo</title><meta name=Description content="LoveIt"><meta property="og:title" content="DataParallel  the Dimension of Output Is Different From Input">
<meta property="og:description" content="torch.nn.DataParallel 问题描述 当使用nn.DataParallel包裹模型后，模型输入的Batch维度和输出的batch维度不一致，此问题尤其会出现在多个输入的">
<meta property="og:type" content="article">
<meta property="og:url" content="https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2021-12-15T16:24:02+08:00">
<meta property="article:modified_time" content="2021-12-15T16:24:02+08:00">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="DataParallel  the Dimension of Output Is Different From Input">
<meta name=twitter:description content="torch.nn.DataParallel 问题描述 当使用nn.DataParallel包裹模型后，模型输入的Batch维度和输出的batch维度不一致，此问题尤其会出现在多个输入的">
<meta name=application-name content="majiabo">
<meta name=apple-mobile-web-app-title content="majiabo"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico>
<link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/><link rel=prev href=https://birkhoffkiki.github.io/hugosite/bilinear_interpolation/><link rel=stylesheet href=/hugosite/lib/normalize/normalize.min.css><link rel=stylesheet href=/hugosite/css/style.min.css><link rel=stylesheet href=/hugosite/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/hugosite/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"DataParallel  the Dimension of Output Is Different From Input","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/birkhoffkiki.github.io\/hugosite\/dataparallel-the-dimension-of-output-is-different-from-input\/"},"genre":"posts","keywords":"pytorch, CV","wordcount":1150,"url":"https:\/\/birkhoffkiki.github.io\/hugosite\/dataparallel-the-dimension-of-output-is-different-from-input\/","datePublished":"2021-12-15T16:24:02+08:00","dateModified":"2021-12-15T16:24:02+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"majiabo"},"description":""}</script></head>
<body header-desktop=fixed header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem('theme')?localStorage.getItem('theme')==='dark':''==='auto'?window.matchMedia('(prefers-color-scheme: dark)').matches:''==='dark')&&document.body.setAttribute('theme','dark')</script>
<div id=mask></div><div class=wrapper><header class=desktop id=header-desktop>
<div class=header-wrapper>
<div class=header-title>
<a href=/hugosite/ title=majiabo></a>
</div>
<div class=menu>
<div class=menu-inner><a class=menu-item href=/hugosite/posts/> 文章 </a><a class=menu-item href=/hugosite/tags/> 标签 </a><a class=menu-item href=/hugosite/categories/> 分类 </a><a class=menu-item href=https://majiabo.top rel="noopener noreffer" target=_blank> 关于 </a><a class=menu-item href=https://hugoloveit.com/zh-cn/ rel="noopener noreffer" target=_blank>&#128512 使用指南 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-desktop>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a>
</div>
</div>
</div>
</header><header class=mobile id=header-mobile>
<div class=header-container>
<div class=header-wrapper>
<div class=header-title>
<a href=/hugosite/ title=majiabo></a>
</div>
<div class=menu-toggle id=menu-toggle-mobile>
<span></span><span></span><span></span>
</div>
</div>
<div class=menu id=menu-mobile><div class=search-wrapper>
<div class="search mobile" id=search-mobile>
<input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索>
<i class="fas fa-search fa-fw"></i>
</a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空>
<i class="fas fa-times-circle fa-fw"></i>
</a>
<span class="search-button search-loading" id=search-loading-mobile>
<i class="fas fa-spinner fa-fw fa-spin"></i>
</span>
</div>
<a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>
取消
</a>
</div><a class=menu-item href=/hugosite/posts/ title>文章</a><a class=menu-item href=/hugosite/tags/ title>标签</a><a class=menu-item href=/hugosite/categories/ title>分类</a><a class=menu-item href=https://majiabo.top title rel="noopener noreffer" target=_blank>关于</a><a class=menu-item href=https://hugoloveit.com/zh-cn/ title rel="noopener noreffer" target=_blank>&#128512使用指南</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
</a></div>
</div>
</header>
<div class="search-dropdown desktop">
<div id=search-dropdown-desktop></div>
</div>
<div class="search-dropdown mobile">
<div id=search-dropdown-mobile></div>
</div>
<main class=main>
<div class=container><div class=toc id=toc-auto>
<h2 class=toc-title>目录</h2>
<div class=toc-content id=toc-content-auto></div>
</div><article class="page single"><h1 class="single-title animated flipInX">DataParallel the Dimension of Output Is Different From Input</h1><div class=post-meta>
<div class=post-meta-line><span class=post-author><a href=https://majiabo.top title=Author target=_blank rel="noopener noreffer author" class=author><i class="fas fa-user-circle fa-fw"></i>majiabo</a></span>&nbsp;<span class=post-category>收录于 <a href=/hugosite/categories/pytorch/><i class="far fa-folder fa-fw"></i>pytorch</a></span></div>
<div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=151512-1215-08>151512-1215-08</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 1150 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 3 分钟&nbsp;</div>
</div><div class="details toc" id=toc-static kept=true>
<div class="details-summary toc-title">
<span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span>
</div>
<div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents>
<ul>
<li><a href=#问题描述>问题描述</a></li>
<li><a href=#原因分析>原因分析</a></li>
<li><a href=#解决方案>解决方案</a></li>
<li><a href=#额外探索实验>额外探索实验</a></li>
</ul>
</nav></div>
</div><div class=content id=content><h1 id=torchnndataparallel>torch.nn.DataParallel</h1>
<h2 id=问题描述>问题描述</h2>
<p>当使用nn.DataParallel包裹模型后，模型输入的Batch维度和输出的batch维度不一致，此问题尤其会出现在多个输入的情况下，且其中某个输入为了节省资源被共用的情况下。
举例说明情况：<br>
假设model正常接受两个输入$x_1 \in [N, C, H, W]$, $x_2\in [1, M, N]$, 且模型输出为$y_1 \in [N, C, H, W]$。 若model被nn.DataParallel包裹后且使用2个GPU时，假设$x_1, x_2$不变，此时输出将变为$y\in [N/2, C, H, W]$。</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=c1># 代码演示</span>
<span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
<span class=kn>import</span> <span class=nn>torch</span>


<span class=k>class</span> <span class=nc>Model</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
        <span class=nb>super</span><span class=p>(</span><span class=n>Model</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
        <span class=k>pass</span>

    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>):</span>
        <span class=c1># implementation details</span>
        <span class=k>return</span> <span class=n>x1</span><span class=p>,</span> <span class=n>x2</span>


<span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
    <span class=kn>import</span> <span class=nn>os</span>
    <span class=n>os</span><span class=o>.</span><span class=n>environ</span><span class=p>[</span><span class=s1>&#39;CUDA_VISIBLE_DEVICES&#39;</span><span class=p>]</span> <span class=o>=</span> <span class=s1>&#39;1,2&#39;</span>
    <span class=n>x1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>10</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>))</span>
    <span class=n>x2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>))</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>()</span>
    <span class=n>dp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>DataParallel</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=n>y1</span><span class=p>,</span> <span class=n>y2</span> <span class=o>=</span> <span class=n>dp</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
    <span class=n>y3</span><span class=p>,</span> <span class=n>y4</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y1: </span><span class=si>{}</span><span class=s1>, y2:</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>y1</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y2</span><span class=o>.</span><span class=n>shape</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y3: </span><span class=si>{}</span><span class=s1>, y4:</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>y3</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y4</span><span class=o>.</span><span class=n>shape</span><span class=p>))</span>
</code></pre></td></tr></table>
</div>
</div><p>output of code:</p>
<blockquote>
<p>torch version: 1.5.0+cu101<br>
y1: torch.Size([5, 3, 128, 128]), y2:torch.Size([1, 100, 200])<br>
y3: torch.Size([10, 3, 128, 128]), y4:torch.Size([1, 100, 200])</p>
</blockquote>
<h2 id=原因分析>原因分析</h2>
<p>主要原因在于输入中的某一个Tensor被复用，但其batch为1导致的。 当model被nn.DataParallel包裹后（假设其为dp)，在执行</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>y1</span><span class=p>,</span> <span class=n>y2</span> <span class=o>=</span> <span class=n>dp</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><p>时，dp模型的$forward$函数会先被调用，我们来看一下其forward函数是怎么构成的:</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=o>*</span><span class=n>inputs</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>autograd</span><span class=o>.</span><span class=n>profiler</span><span class=o>.</span><span class=n>record_function</span><span class=p>(</span><span class=s2>&#34;DataParallel.forward&#34;</span><span class=p>):</span>
            <span class=k>if</span> <span class=ow>not</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_ids</span><span class=p>:</span>
                <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>module</span><span class=p>(</span><span class=o>*</span><span class=n>inputs</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>

            <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>chain</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>module</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=bp>self</span><span class=o>.</span><span class=n>module</span><span class=o>.</span><span class=n>buffers</span><span class=p>()):</span>
                <span class=k>if</span> <span class=n>t</span><span class=o>.</span><span class=n>device</span> <span class=o>!=</span> <span class=bp>self</span><span class=o>.</span><span class=n>src_device_obj</span><span class=p>:</span>
                    <span class=k>raise</span> <span class=ne>RuntimeError</span><span class=p>(</span><span class=s2>&#34;module must have its parameters and buffers &#34;</span>
                                       <span class=s2>&#34;on device </span><span class=si>{}</span><span class=s2> (device_ids[0]) but found one of &#34;</span>
                                       <span class=s2>&#34;them on device: </span><span class=si>{}</span><span class=s2>&#34;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>src_device_obj</span><span class=p>,</span> <span class=n>t</span><span class=o>.</span><span class=n>device</span><span class=p>))</span>
            <span class=c1># 此处会对输入在dim=0上进行拆分，</span>
            <span class=c1># 有几块卡就拆分为几份，因为我们的x2的第一个维度（dim=0）为一</span>
            <span class=c1>#无法再进行拆分，导致了此问题，可进一步探寻self.catter具体是如何工作的</span>
            <span class=n>inputs</span><span class=p>,</span> <span class=n>kwargs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>kwargs</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>device_ids</span><span class=p>)</span> 
            <span class=c1># ... 其他code为节省空间，省略</span>

</code></pre></td></tr></table>
</div>
</div><p>上述代码中，$self.scatter$函数会对所有输入数据按比例进行拆分，有多少块卡每个输入就会在dim=0上被拆分为多少份。而例子中$x2$第一维为1，无法进一步拆分，所以虽然$x1$的拆分有两个，最终能够匹配的份数还是一。个人感觉与下列代码功能类似：</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>x1</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span>
<span class=n>x2</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>]</span>
<span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>):</span>
    <span class=c1># 只会循环一次</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>i</span><span class=p>,</span> <span class=n>j</span><span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id=解决方案>解决方案</h2>
<p>将共用的Tensor的0维度翻倍，建议使用expand改变Tensor的view(view是Tensor的视图，可看成Tensor的维度)。expand不会对Tensor进行复制，相比于repeat能够节省内存。</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python>
    <span class=n>x1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>10</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>))</span>
    <span class=n>x2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>))</span>
    <span class=c1># 对x2的view进行调整，将其调整为GPU_num的倍数，以2为例</span>
    <span class=n>x2</span> <span class=o>=</span> <span class=n>x2</span><span class=o>.</span><span class=n>expand</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>()</span>
    <span class=n>dp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>DataParallel</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=n>y1</span><span class=p>,</span> <span class=n>y2</span> <span class=o>=</span> <span class=n>dp</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
    <span class=n>y3</span><span class=p>,</span> <span class=n>y4</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y1: </span><span class=si>{}</span><span class=s1>, y2:</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>y1</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y2</span><span class=o>.</span><span class=n>shape</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y3: </span><span class=si>{}</span><span class=s1>, y4:</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>y3</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y4</span><span class=o>.</span><span class=n>shape</span><span class=p>))</span>

</code></pre></td></tr></table>
</div>
</div><p>outputs:</p>
<blockquote>
<p>torch version: 1.5.0+cu101<br>
y1: torch.Size([10, 3, 128, 128]), y2:torch.Size([2, 100, 200])<br>
y3: torch.Size([10, 3, 128, 128]), y4:torch.Size([2, 100, 200])</p>
</blockquote>
<h2 id=额外探索实验>额外探索实验</h2>
<p>将x2的batch维度改为3</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python>    <span class=n>x1</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>10</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>))</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
    <span class=n>x2</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>200</span><span class=p>))</span><span class=o>.</span><span class=n>cuda</span><span class=p>()</span>
    <span class=n>model</span> <span class=o>=</span> <span class=n>Model</span><span class=p>()</span>
    <span class=n>dp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>DataParallel</span><span class=p>(</span><span class=n>model</span><span class=p>)</span>
    <span class=n>y1</span><span class=p>,</span> <span class=n>y2</span> <span class=o>=</span> <span class=n>dp</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
    <span class=n>y3</span><span class=p>,</span> <span class=n>y4</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=n>x1</span><span class=p>,</span> <span class=n>x2</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;torch version:&#39;</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>__version__</span><span class=p>)</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y1: </span><span class=si>{}</span><span class=s1>, y2:</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>y1</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y2</span><span class=o>.</span><span class=n>shape</span><span class=p>))</span>
    <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;y3: </span><span class=si>{}</span><span class=s1>, y4:</span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>y3</span><span class=o>.</span><span class=n>shape</span><span class=p>,</span> <span class=n>y4</span><span class=o>.</span><span class=n>shape</span><span class=p>))</span>
</code></pre></td></tr></table>
</div>
</div><p>outputs:</p>
<blockquote>
<p>torch version: 1.5.0+cu101<br>
y1: torch.Size([10, 3, 128, 128]), y2:torch.Size([3, 100, 200])<br>
y3: torch.Size([10, 3, 128, 128]), y4:torch.Size([3, 100, 200])</p>
</blockquote>
<p><strong>注意</strong> 此时$x1$,$x2$的划分为</p>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=n>GPU0</span><span class=p>:</span> <span class=n>x1</span> <span class=n>shape</span> <span class=ow>is</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>],</span> <span class=n>x2</span> <span class=n>shape</span> <span class=ow>is</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>100</span><span class=p>]</span>
<span class=n>GPU1</span><span class=p>:</span> <span class=n>x2</span> <span class=n>shape</span> <span class=ow>is</span> <span class=p>[</span><span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>],</span> <span class=n>x2</span> <span class=n>shape</span> <span class=ow>is</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>100</span><span class=p>,</span> <span class=mi>100</span><span class=p>]</span>
</code></pre></td></tr></table>
</div>
</div></div><div class=post-footer id=post-footer>
<div class=post-info>
<div class=post-info-line>
<div class=post-info-mod>
<span>更新于 151512-1215-08</span>
</div>
<div class=post-info-license></div>
</div>
<div class=post-info-line>
<div class=post-info-md><span>
<a class=link-to-markdown href=/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/index.md target=_blank>阅读原始文档</a>
</span></div>
<div class=post-info-share>
<span><a href=javascript:void(0); title="分享到 Twitter" data-sharer=twitter data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input" data-hashtags=pytorch,CV><i class="fab fa-twitter fa-fw"></i></a><a href=javascript:void(0); title="分享到 Facebook" data-sharer=facebook data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-hashtag=pytorch><i class="fab fa-facebook-square fa-fw"></i></a><a href=javascript:void(0); title="分享到 WhatsApp" data-sharer=whatsapp data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href=javascript:void(0); title="分享到 Line" data-sharer=line data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input"><i data-svg-src=/hugosite/lib/simple-icons/icons/line.min.svg></i></a><a href=javascript:void(0); title="分享到 微博" data-sharer=weibo data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input"><i class="fab fa-weibo fa-fw"></i></a><a href=javascript:void(0); title="分享到 Myspace" data-sharer=myspace data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input" data-description><i data-svg-src=/hugosite/lib/simple-icons/icons/myspace.min.svg></i></a><a href=javascript:void(0); title="分享到 Blogger" data-sharer=blogger data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input" data-description><i class="fab fa-blogger fa-fw"></i></a><a href=javascript:void(0); title="分享到 Evernote" data-sharer=evernote data-url=https://birkhoffkiki.github.io/hugosite/dataparallel-the-dimension-of-output-is-different-from-input/ data-title="DataParallel  the Dimension of Output Is Different From Input"><i class="fab fa-evernote fa-fw"></i></a></span>
</div>
</div>
</div>
<div class=post-info-more>
<section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/hugosite/tags/pytorch/>pytorch</a>,&nbsp;<a href=/hugosite/tags/cv/>CV</a></section>
<section>
<span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/hugosite/>主页</a></span>
</section>
</div>
<div class=post-nav><a href=/hugosite/bilinear_interpolation/ class=prev rel=prev title="Bilinear_interpolation 原理及实现"><i class="fas fa-angle-left fa-fw"></i>Bilinear_interpolation 原理及实现</a></div>
</div>
<div id=comments><div id=gitalk class=comment></div><noscript>
Please enable JavaScript to view the comments powered by <a href=https://github.com/gitalk/gitalk></a>Gitalk</a>.
</noscript></div></article></div>
</main><footer class=footer>
<div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.90.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
</div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2021</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=https://majiabo.top target=_blank>majiabo</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div>
</div>
</footer></div>
<div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部>
<i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论>
<i class="fas fa-comment fa-fw"></i>
</a>
</div><link rel=stylesheet href=/hugosite/lib/gitalk/gitalk.min.css><link rel=stylesheet href=/hugosite/lib/katex/katex.min.css><link rel=stylesheet href=/hugosite/lib/katex/copy-tex.min.css><link rel=stylesheet href=/hugosite/lib/cookieconsent/cookieconsent.min.css><script type=text/javascript src=/hugosite/lib/gitalk/gitalk.min.js></script><script type=text/javascript src=/hugosite/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/hugosite/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/hugosite/lib/lunr/lunr.min.js></script><script type=text/javascript src=/hugosite/lib/lunr/lunr.stemmer.support.min.js></script><script type=text/javascript src=/hugosite/lib/lunr/lunr.zh.min.js></script><script type=text/javascript src=/hugosite/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/hugosite/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/hugosite/lib/sharer/sharer.min.js></script><script type=text/javascript src=/hugosite/lib/katex/katex.min.js></script><script type=text/javascript src=/hugosite/lib/katex/auto-render.min.js></script><script type=text/javascript src=/hugosite/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/hugosite/lib/katex/mhchem.min.js></script><script type=text/javascript src=/hugosite/lib/cookieconsent/cookieconsent.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:10},comment:{gitalk:{admin:["birkhoffkiki"],clientID:"d6b047b05a271ae1181f",clientSecret:"8c774ac612503288bc0e50802e52a87e95d5a06c",id:"2021-12-15T16:24:02+08:00",owner:"birkhoffkiki",repo:"hugosite",title:"DataParallel  the Dimension of Output Is Different From Input"}},cookieconsent:{content:{dismiss:"同意",link:"了解更多",message:"本网站使用 Cookies 来改善您的浏览体验."},enable:!0,palette:{button:{background:"#f0f0f0"},popup:{background:"#1aa3ff"}},theme:"edgeless"},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/hugosite/index.json",lunrLanguageCode:"zh",lunrSegmentitURL:"/hugosite/lib/lunr/lunr.segmentit.js",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"lunr"}}</script><script type=text/javascript src=/hugosite/js/theme.min.js></script></body>
</html>